{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f61dae3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e01a49d37784d6e82c5203c87bb9329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff86deeb5544151bb13ba83740543aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "############ Tensorflow에서 제공된 Type별 Feature 생성 코드 ############\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    # string / byte 타입을 받아서 byte list를 리턴.\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    # float / double 타입을 받아서 float list를 리턴\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _floatarray_feature(array):\n",
    "    # float / double 타입을 받아서 float list를 리턴\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=array))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    # bool / enum / int / uint 타입을 받아서 int64 list를 리턴\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def to_example(filename):\n",
    "    # TFRecord를 생성하는 시점에 resize를 할수도 있고 TFRecord를 읽어서 DataSet을 만들때\n",
    "    # resize를 할 수 도 있다. \n",
    "    # resize 된 이미지를 저장할 경우 처리에 주의해야 한다. \n",
    "    # 잘못하는 경우 이미지 데이터가 깨져서 저장될 수 있음.\n",
    "    # 일반적으로 원본을 저장하고 사용할 때 resize해서 사용하는것이 일반적임.\n",
    "            \n",
    "    # 원본 이미지를 resize하지 않고 TFRecord로 저장.\n",
    "    image_string = tf.io.read_file(filename)    \n",
    "    \n",
    "    # './data/kaggle_cat_dog/train/cat.12406.jpg'\n",
    "    label = (filename.split('/')[-1]).split('.')[0] == 'dog'\n",
    "    # cat이면 False(0), dog이면 True(1)\n",
    "    \n",
    "    shape = tf.image.decode_jpeg(image_string).shape\n",
    "\n",
    "    feature = {\n",
    "        'image/height': _int64_feature(shape[0]),\n",
    "        'image/width': _int64_feature(shape[1]),\n",
    "        'image/channel': _int64_feature(shape[2]),\n",
    "        'image/label': _int64_feature(label),\n",
    "        'image/image_raw': _bytes_feature(image_string),\n",
    "        'image/filename': _bytes_feature(filename.encode())        \n",
    "    }\n",
    "    \n",
    "    # Example 객체 생성\n",
    "    # 파이썬의 문자열은 모두 unicode. unicode 문자열을 byte배열로 바꾸는 함수가 encode()\n",
    "    # 각 type에 맞게 Feature 객체 생성 후 dict 생성. \n",
    "    # 이 dict를 이용해 Feauture 객체 생성 후 Example 객체 생성.\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "def chunkify(filename_list, n):\n",
    "    size = len(filename_list) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n - 1):\n",
    "        results.append(filename_list[start:start + size])\n",
    "        start += size\n",
    "    results.append(filename_list[start:])\n",
    "    return results\n",
    "\n",
    "\n",
    "# tqdm은 반복문의 진행상황을 progressbar를 보여주는 모듈\n",
    "\n",
    "# tqdm 파라미터\n",
    "# iterable : 반복자 객체\n",
    "# desc : 진행바 앞에 텍스트 출력\n",
    "# total : int, 전체 반복량\n",
    "# leave : bool, default로 True. (진행상태 잔상이 남음)\n",
    "# ncols : 진행바 컬럼길이. width 값으로 pixel 단위로 보임.\n",
    "# mininterval, maxinterval : 업데이트 주기. \n",
    "#                            기본은 mininterval=0.1 sec, maxinterval=10 sec\n",
    "# miniters : Minimum progress display update interval, in iterations.\n",
    "# ascii : True로 하면 '#'문자로 진행바가 표시됨.\n",
    "# initial : 진행 시작값. 기본은 0\n",
    "# bar_format : str\n",
    "\n",
    "# tqdm method\n",
    "# clear() : 삭제\n",
    "# refresh() : 강제 갱신\n",
    "        \n",
    "def build_tfrecords(total_shards_num, split, filenames):\n",
    "    chunks = chunkify(filenames, total_shards_num)\n",
    "    failed = 0\n",
    "    for i, chunk in tqdm(enumerate(chunks),\n",
    "                         total=len(chunks),\n",
    "                         position=0,\n",
    "                         leave=True):        \n",
    "        tfrecords_path = './tfrecords/{}_{}_of_{}.tfrecords'.format(split,\n",
    "                                                                    str(i + 1).zfill(4),\n",
    "                                                                    str(total_shards_num).zfill(4))\n",
    "        # GZIP으로 압축한 TFRecord 생성하기 위한 option\n",
    "        # options = tf.io.TFRecordOptions(compression_type='GZIP')\n",
    "        \n",
    "        # with tf.io.TFRecordWriter(tfrecords_path, options=options) as writer:\n",
    "        with tf.io.TFRecordWriter(tfrecords_path) as writer:\n",
    "            for filename in chunk:\n",
    "                try:\n",
    "                    tf_example = to_example(filename)\n",
    "                    \n",
    "                    # 만들어진 Example 객체를 binary string으로 변환한 후 파일에 저장\n",
    "                    writer.write(tf_example.SerializeToString())\n",
    "                except:\n",
    "                    print(f'fail: {filename}')\n",
    "    \n",
    "    \n",
    "os.makedirs('tfrecords', exist_ok=True)\n",
    "filenames = tf.io.gfile.glob('../../data/dogs-vs-cats/train/*.jpg')\n",
    "\n",
    "train_data, valid_data = train_test_split(filenames, train_size=0.8)\n",
    "\n",
    "build_tfrecords(100, 'train', train_data)\n",
    "build_tfrecords(100, 'valid', valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b73aed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db21a764ed674672978105df89354d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_acc improved from -inf to 1.00000, saving model to ./checkpoint\\cat-dog-000001-1.000000-0.990000.hdf5\n",
      "\n",
      "Epoch 2: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 3: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 4: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 5: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 6: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 7: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 8: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 9: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 10: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 11: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 12: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 13: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 14: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 15: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 16: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 17: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 18: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 19: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 20: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 1: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 2: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 3: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 4: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 5: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 6: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 7: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 8: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 9: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 10: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 11: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 12: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 13: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 14: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 15: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 16: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 17: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 18: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 19: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 20: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 1: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 2: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 3: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 4: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 5: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 6: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 7: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 8: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 9: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 10: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 11: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 12: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 13: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 14: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 15: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 16: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 17: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 18: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 19: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 20: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 1: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 2: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 3: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 4: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 5: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 6: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 7: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 8: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 9: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 10: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 11: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 12: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 13: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 14: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 15: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 16: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 17: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 18: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 19: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 20: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 1: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 2: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 3: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 4: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 5: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 6: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 7: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 8: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 9: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 10: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 11: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 12: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 13: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 14: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 15: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 16: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 17: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 18: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 19: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 20: val_acc did not improve from 1.00000\n",
      "############### 기본학습 종료 ###############\n",
      "########### 재학습 진행 ###########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c5c3627ea0472dafc8001ce6e1f938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 2: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 3: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 4: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 5: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 6: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 7: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 8: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 9: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 10: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 11: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 12: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 13: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 14: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 15: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 16: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 17: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 18: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 19: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 20: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 1: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 2: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 3: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 4: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 5: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 6: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 7: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 8: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 9: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 10: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 11: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 12: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 13: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 14: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 15: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 16: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 17: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 18: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 19: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 20: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 1: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 2: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 3: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 4: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 5: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 6: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 7: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 8: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 9: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 10: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 11: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 12: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 13: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 14: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 15: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 16: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 17: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 18: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 19: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 20: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 1: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 2: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 3: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 4: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 5: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 6: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 7: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 8: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 9: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 10: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 11: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 12: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 13: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 14: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 15: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 16: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 17: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 18: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 19: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 20: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 1: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 2: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 3: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 4: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 5: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 6: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 7: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 8: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 9: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 10: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 11: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 12: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 13: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 14: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 15: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 16: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 17: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 18: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 19: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 20: val_acc did not improve from 1.00000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# 학습에 필요한 DataSet 준비(여러개의 tfrecord 처리)\n",
    "\n",
    "# train, validation TFRecord 폴더 경로(여러개의 tfrecord 처리)\n",
    "# 폴더를 나누고 파일을 복사하는건 수동으로 처리\n",
    "train_tfrecord_path = './tfrecords/train'\n",
    "valid_tfrecord_path = './tfrecords/valid'\n",
    "\n",
    "train_tfrecord_list = tf.io.gfile.glob(train_tfrecord_path + '/*.tfrecords')\n",
    "valid_tfrecord_list = tf.io.gfile.glob(valid_tfrecord_path + '/*.tfrecords')\n",
    "\n",
    "# 읽어들인 TFRecord를 다음의 형태(dict)로 변환하는 함수\n",
    "# <ParallelMapDataset shapes: {id: (), image_raw: (), label: ()}, \n",
    "#                     types: {id: tf.string, image_raw: tf.string, label: tf.int64}>\n",
    "def _parse_image_function(example_proto):\n",
    "    # TFRecord를 읽어서 데이터를 복원하기 위한 자료구조.\n",
    "    image_feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/channel': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    return tf.io.parse_single_example(example_proto, \n",
    "                                      image_feature_description)\n",
    "\n",
    "# 위에서 얻은 ParallelMapDataset를 다음의 형태(shape)로 변환하는 함수\n",
    "# <ParallelMapDataset shapes: ((None, None, 3), ()), types: (tf.float32, tf.int64)>\n",
    "def map_func(target_record):      \n",
    "    img = target_record['image/image_raw']\n",
    "    label = target_record['image/label']\n",
    "    img = tf.image.decode_jpeg(img, channels=3)    \n",
    "    return img, label\n",
    "\n",
    "\n",
    "# 전처리(normalization & resize) 함수\n",
    "# 이미지 데이터 normalization\n",
    "# 우리예제는 TFRecord 생성 시 원본 size로 저장했기 때문에 image resize를 해야함.\n",
    "def image_preprocess_func(image, label):\n",
    "    result_image = image / 255\n",
    "    result_image = tf.image.resize(result_image, \n",
    "                                   (IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                   antialias=False)   \n",
    "    return result_image, label\n",
    "\n",
    "\n",
    "# 만약 multinomial classification이면 one_hot처리도 필요함.\n",
    "def image_postprocess_func(image, label):\n",
    "#    onehot_label = tf.one_hot(label, depth=1049)    # binary인 경우 one_hot 사용안함.    \n",
    "    return image, label\n",
    "\n",
    "\n",
    "def make_dataset(tfrecords_path, is_train):\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(tfrecords_path)\n",
    "\n",
    "    dataset = dataset.map(_parse_image_function,\n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    dataset = dataset.map(map_func,\n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "    dataset = dataset.map(image_preprocess_func,\n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    dataset = dataset.map(image_postprocess_func,\n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "############### Parameter\n",
    "NUM_OF_TFRECORDS = 5 # 종류별 TFRecord의 개수\n",
    "BUFFER_SIZE = 16     # 데이터 shuffle을 위한 buffer size\n",
    "BATCH_SIZE = 8       # 배치 사이즈. 한번에 가져오는 이미지 데이터 개수 \n",
    "NUM_CLASS = 2        # class의 개수. binary인 경우는 필요없으며 categorical인 경우 설정\n",
    "IMAGE_SIZE = 150       \n",
    "\n",
    "############### Model\n",
    "\n",
    "input_layer = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name='input_layer')  \n",
    "\n",
    "## Pretrained Network\n",
    "pretrained_model = VGG16(weights='imagenet',\n",
    "                         include_top=False,\n",
    "                         input_shape=(IMAGE_SIZE,IMAGE_SIZE,3),\n",
    "                         input_tensor=input_layer)\n",
    "\n",
    "pretrained_model.trainable = False\n",
    "\n",
    "x = pretrained_model.output\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(256,\n",
    "          activation='relu')(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "\n",
    "# EarlyStopping & Checkpoint & Learning Rate\n",
    "learning_rate_reduction=ReduceLROnPlateau(\n",
    "                        monitor= \"val_acc\", \n",
    "                        patience = 3, \n",
    "                        factor = 0.5, \n",
    "                        min_lr=1e-7,\n",
    "                        verbose=1)\n",
    "\n",
    "model_filename = './checkpoint/cat-dog-{epoch:06d}-{val_acc:0.6f}-{acc:0.6f}.hdf5'\n",
    "\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath=model_filename, \n",
    "    verbose=1, \n",
    "    save_freq='epoch', \n",
    "    save_best_only=True, \n",
    "    monitor='val_acc')\n",
    "\n",
    "es = EarlyStopping(monitor='val_acc', verbose=1, patience=5)\n",
    "\n",
    "# LearningRateScheduler 이용\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "\n",
    "\n",
    "# 데이터 세팅 후 학습(여러개(100개)의 tfrecord를 이용한 학습)\n",
    "for i in tqdm(range(NUM_OF_TFRECORDS),\n",
    "              total=NUM_OF_TFRECORDS,\n",
    "              position=0,\n",
    "              leave=True):\n",
    "    tfrecord_train_file = train_tfrecord_list[i]\n",
    "    tfrecord_valid_file = valid_tfrecord_list[i]\n",
    "    \n",
    "    dataset = make_dataset(tfrecord_train_file, True)\n",
    "    valid_dataset = make_dataset(tfrecord_valid_file, False)\n",
    "    \n",
    "    model.fit(dataset,\n",
    "              epochs=20,\n",
    "              validation_data=valid_dataset,\n",
    "#               callbacks = [checkpointer, es, learning_rate_reduction],\n",
    "              callbacks = [checkpointer],\n",
    "              verbose=0)       \n",
    "\n",
    "print('############### 기본학습 종료 ###############')    \n",
    "\n",
    "# 여기까지가 기본학습 처리입니다.\n",
    "\n",
    "# Pretrained Network 위에 새로운 Network을 추가합니다.\n",
    "# Base Network을 동결합니다.\n",
    "# 새로 추가한 Network을 학습합니다.\n",
    "\n",
    "\n",
    "# 아래의 작업이 추가로 필요합니다.\n",
    "# Base Network에서 일부 Layer의 동결을 해제합니다.\n",
    "# 동결을 해제한 층과 새로 추가한 층을 함께 학습합니다.\n",
    "\n",
    "pretrained_model.trainable = True\n",
    "\n",
    "for layer in pretrained_model.layers:\n",
    "    if layer.name in ['block5_conv1','block5_conv2','block5_conv3']:\n",
    "        layer.trainable = True\n",
    "    else:    \n",
    "        layer.trainable = False\n",
    "\n",
    "## learning rate를 줄이는게 일반적(미세조절)        \n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "    \n",
    "    \n",
    "    \n",
    "## 재학습 진행\n",
    "print('########### 재학습 진행 ###########')\n",
    "\n",
    "for i in tqdm(range(NUM_OF_TFRECORDS),\n",
    "              total=NUM_OF_TFRECORDS,\n",
    "              position=0,\n",
    "              leave=True):\n",
    "\n",
    "    tfrecord_train_file = train_tfrecord_list[i]\n",
    "    tfrecord_valid_file = valid_tfrecord_list[i]\n",
    "    \n",
    "    dataset = make_dataset(tfrecord_train_file, True)\n",
    "    valid_dataset = make_dataset(tfrecord_valid_file, False)\n",
    "    \n",
    "    model.fit(dataset,\n",
    "              epochs=20,\n",
    "              validation_data=valid_dataset,\n",
    "#               callbacks = [checkpointer, es, learning_rate_reduction],\n",
    "              callbacks = [checkpointer],\n",
    "              verbose=0)            \n",
    "\n",
    "# 결과그래프(loss, accuracy)를 그리는것은 한번 고민해보시기 바랍니다.!!\n",
    "\n",
    "# 프로그램 수행환경 \n",
    "# Server : AWS EC2 2.4.1\n",
    "#          1 GPU(Tesla T4), 16GB GPU Memory\n",
    "# CUDA : 11.0.4, cuDNN : 8\n",
    "# Tensorflow-gpu : 2.4.1\n",
    " \n",
    "# 최종 저장된 모델\n",
    "\n",
    "# 5개의 TFRecord로 학습한 경우\n",
    "# Epoch 00016: val_acc improved from 0.95300 to 0.95400, \n",
    "# saving model to ./checkpoint/cat-dog-000016-0.954000-1.000000.hdf5\n",
    "\n",
    "# 1개의 TFRecord로 학습한 경우\n",
    "# Epoch 00015: val_acc improved from 0.95140 to 0.95420, \n",
    "# saving model to ./checkpoint/cat-dog-000015-0.954200-0.995150.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f080a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine_TF2] *",
   "language": "python",
   "name": "conda-env-machine_TF2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
